import math
import sys
import os
import re

# Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø§Ù†Ø¯ÙƒØ³ Ø§Ù„Ù†Ø§ØªØ¬ Ù…Ù† Ø³Ø¨Ø§Ø±Ùƒ
INPUT_FILE = "output.txt"

def load_index(file_path):
    """
    ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø¯Ø§Ù„Ø©: Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù output.txt ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§ÙŠØ«ÙˆÙ†
    ØªØ³ØªØ®Ø¯Ù… Regex Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    """
    print(f"Loading index from {file_path}...")
    index = {}
    all_docs = set()
    
    if not os.path.exists(file_path):
        print(f"âŒ Error: File '{file_path}' not found.")
        print("Please run the Spark Indexer first to generate this file.")
        return None, None

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f):
            line = line.strip()
            if not line: continue
            
            # Regex Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø³Ø·Ø± Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†:
            # ÙŠØ¨Ø­Ø« Ø¹Ù†: < term : rest_of_line >
            # Ø§Ù„Ù†Ù…Ø· ÙŠØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            match = re.match(r"^<\s*(.*?)\s*:\s*(.*)\s*>$", line)
            
            if not match:
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ®Ø·ÙŠ Ø§Ù„Ø³Ø·ÙˆØ± Ø§Ù„ØªØ§Ù„ÙØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
                continue
                
            term = match.group(1).strip()
            docs_str = match.group(2).strip()
            
            index[term] = {}
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨ÙØ§ØµÙ„Ø© Ù…Ù†Ù‚ÙˆØ·Ø©
            # Format: doc1: pos1, pos2 ; doc2: pos1 ...
            doc_entries = docs_str.split(';')
            
            for entry in doc_entries:
                if ':' not in entry: continue
                
                doc_part, pos_part = entry.split(':', 1)
                doc_name = doc_part.strip()
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹)
                positions = [int(p) for p in re.findall(r'\d+', pos_part)]
                
                if doc_name and positions:
                    index[term][doc_name] = positions
                    all_docs.add(doc_name)
                    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (1.txt, 2.txt...) Ù„Ø¶Ù…Ø§Ù† Ø´ÙƒÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    # Ù†Ù‚ÙˆÙ… Ø¨Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù‚Ù… Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­
    def sort_key(doc):
        nums = re.findall(r'\d+', doc)
        return int(nums[0]) if nums else doc
        
    sorted_docs = sorted(list(all_docs), key=sort_key)
    
    print(f"âœ… Successfully loaded {len(index)} terms and {len(sorted_docs)} documents.")
    return index, sorted_docs

def print_table(title, headers, rows):
    """ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù… """
    print(f"\nğŸ“Š --- {title} ---")
    
    # Ø­Ø³Ø§Ø¨ Ø¹Ø±Ø¶ ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø·ÙˆÙ„ ÙƒÙ„Ù…Ø© ÙÙŠÙ‡
    col_widths = [len(h) for h in headers]
    for row in rows:
        for i, val in enumerate(row):
            if i < len(col_widths):
                col_widths[i] = max(col_widths[i], len(str(val)))
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§ÙØ© Ø¬Ù…Ø§Ù„ÙŠØ©
    col_widths = [w + 2 for w in col_widths]
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø±Ø£Ø³
    header_str = "".join(f"{h:<{col_widths[i]}}" for i, h in enumerate(headers))
    print(header_str)
    print("-" * len(header_str))
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØµÙÙˆÙ
    for row in rows:
        print("".join(f"{str(val):<{col_widths[i]}}" for i, val in enumerate(row)))

def main():
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    index, all_docs = load_index(INPUT_FILE)
    if index is None: return

    terms = sorted(index.keys())
    N = len(all_docs) # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙ„ÙŠ

    # ==========================================
    # 2. Ø­Ø³Ø§Ø¨ Term Frequency (TF)
    # ==========================================
    print("\n[1] Computing Term Frequency (TF)...")
    
    tf_matrix = {} # Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø§Ø­Ù‚Ø§Ù‹
    table_rows = []
    
    for term in terms:
        row = [term]
        tf_matrix[term] = {}
        for doc in all_docs:
            # TF Ù‡Ùˆ Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø© (Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹)
            count = len(index[term].get(doc, []))
            tf_matrix[term][doc] = count
            row.append(count)
        table_rows.append(row)
        
    print_table("Term Frequency (TF)", ["Term"] + all_docs, table_rows)

    # ==========================================
    # 3. Ø­Ø³Ø§Ø¨ Inverse Document Frequency (IDF)
    # ==========================================
    print("\n[2] Computing IDF...")
    
    idf_dict = {}
    table_rows = []
    
    for term in terms:
        df = len(index[term]) # ÙÙŠ ÙƒÙ… Ù…Ù„Ù Ø¸Ù‡Ø±Øª Ø§Ù„ÙƒÙ„Ù…Ø©ØŸ
        # Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†: log10( N / df )
        idf = math.log10(N / df) if df > 0 else 0
        idf_dict[term] = idf
        table_rows.append([term, f"{idf:.4f}"])
        
    print_table("IDF Values", ["Term", "IDF"], table_rows)

    # ==========================================
    # 4. Ø­Ø³Ø§Ø¨ TF-IDF Matrix
    # ==========================================
    print("\n[3] Computing TF-IDF Matrix...")
    
    tf_idf_matrix = {}
    table_rows = []
    doc_norms = {doc: 0.0 for doc in all_docs} # Ù„ØªØ¬Ù‡ÙŠØ² Cosine Similarity
    
    for term in terms:
        row = [term]
        tf_idf_matrix[term] = {}
        for doc in all_docs:
            tf = tf_matrix[term][doc]
            idf = idf_dict[term]
            
            # Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†: TF * IDF
            val = tf * idf
            tf_idf_matrix[term][doc] = val
            row.append(f"{val:.4f}")
            
            # ØªØ¬Ù…ÙŠØ¹ Ù…Ø±Ø¨Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ù„Ø­Ø³Ø§Ø¨ Ø·ÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ù‡ (Vector Norm) Ù„Ù„Ù…Ù„Ù
            doc_norms[doc] += val ** 2
            
        table_rows.append(row)
        
    # Ø£Ø®Ø° Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ù„Ù€ Norms
    for doc in doc_norms:
        doc_norms[doc] = math.sqrt(doc_norms[doc])
        
    print_table("TF-IDF Matrix", ["Term"] + all_docs, table_rows)

    # ==========================================
    # 5. Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« (Search Engine)
    # ==========================================
    print("\nğŸ” --- Search Engine Ready ---")
    print("Example queries: 'angels fools', 'antony AND brutus', 'rush AND NOT fear'")
    
    while True:
        try:
            query = input("\nQuery > ").strip()
            if query.lower() in ['exit', 'quit']:
                break
            if not query: continue
            
            # --- 5.1 ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (Parsing) ---
            must_include = []
            must_exclude = []
            
            # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ AND NOT Ø£ÙˆÙ„Ø§Ù‹
            if ' AND NOT ' in query:
                parts = query.split(' AND NOT ')
                must_include.append(parts[0].strip())
                must_exclude.append(parts[1].strip())
            elif ' AND ' in query:
                parts = query.split(' AND ')
                must_include.extend([p.strip() for p in parts])
            elif query.startswith('NOT '):
                must_exclude.append(query[4:].strip())
            else:
                must_include.append(query) # Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©
            
            # --- 5.2 Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Boolean Logic & Phrase) ---
            # Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…Ù„Ø© (Phrase Query)
            def find_phrase_docs(phrase_text):
                p_terms = phrase_text.lower().split()
                if not p_terms: return set()
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
                for t in p_terms:
                    if t not in index: return set()
                
                # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© (Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙƒÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª)
                candidates = set(index[p_terms[0]].keys())
                for t in p_terms[1:]:
                    candidates &= set(index[t].keys())
                
                matched = set()
                for doc in candidates:
                    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ±ØªÙŠØ¨ (Positions)
                    curr_pos = index[p_terms[0]][doc]
                    for i in range(1, len(p_terms)):
                        next_pos = index[p_terms[i]][doc]
                        # Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ù…ÙˆÙ‚Ø¹ (Ø³Ø§Ø¨Ù‚ + 1)ØŸ
                        curr_pos = [p+1 for p in curr_pos if (p+1) in next_pos]
                        if not curr_pos: break
                    if curr_pos: matched.add(doc)
                return matched

            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ù†Ø·Ù‚
            result_docs = None
            
            # Include
            if must_include:
                for phrase in must_include:
                    docs = find_phrase_docs(phrase)
                    if result_docs is None: result_docs = docs
                    else: result_docs &= docs # ØªÙ‚Ø§Ø·Ø¹ (AND)
            else:
                result_docs = set() # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… NOT ÙÙ‚Ø·ØŒ Ù†ÙØªØ±Ø¶ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒÙ„ (Ø£Ùˆ ÙØ§Ø±Øº Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚)
            
            # Exclude
            for phrase in must_exclude:
                docs = find_phrase_docs(phrase)
                if result_docs is None: result_docs = set(all_docs)
                result_docs -= docs # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯
                
            if not result_docs:
                print("No documents found.")
                continue

            # --- 5.3 Ø§Ù„ØªØ±ØªÙŠØ¨ (Ranking - Cosine Similarity) ---
            # ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªØ¬Ù‡
            query_terms = []
            for phrase in must_include:
                query_terms.extend(phrase.lower().split())
            
            # Ø­Ø³Ø§Ø¨ ÙˆØ²Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            # TF query = 1 (Ù„Ù„ØªØ¨Ø³ÙŠØ·)
            q_vec = {}
            q_norm = 0
            for t in query_terms:
                if t in idf_dict:
                    w = idf_dict[t] # (1 + log(1)) * idf = idf
                    q_vec[t] = w
                    q_norm += w**2
            q_norm = math.sqrt(q_norm)
            
            # Ø­Ø³Ø§Ø¨ Cosine Similarity
            scores = []
            for doc in result_docs:
                dot_product = 0
                for t, w in q_vec.items():
                    if t in tf_idf_matrix:
                        dot_product += w * tf_idf_matrix[t][doc]
                
                sim = 0
                if q_norm > 0 and doc_norms[doc] > 0:
                    sim = dot_product / (q_norm * doc_norms[doc])
                
                scores.append((doc, sim))
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø±ØªØ¨Ø©
            scores.sort(key=lambda x: x[1], reverse=True)
            print(f"\nFound {len(scores)} documents:")
            for doc, score in scores:
                print(f"ğŸ“„ {doc:<10} (Score: {score:.4f})")

        except Exception as e:
            print(f"Error processing query: {e}")

if __name__ == "__main__":
    main()